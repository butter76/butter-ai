model:
  n_layer: 6
  n_head: 8
  n_embd: 256
  dropout: 0.1
  max_seq_len: 256
  vocab_size: 37  # Matches the tokenizer's vocabulary size
  # Model size: ~4.8M parameters, ~19MB on disk
  # Breakdown:
  # - Token embeddings: 9,472 params
  # - Position embeddings: 65,536 params
  # - 6 transformer blocks: 4,724,736 params
  # - Final layer norm: 512 params
  # - Output head: 9,472 params
  #
  # Estimated VRAM usage during training (batch_size=32):
  # - Model parameters: ~19MB
  # - Optimizer states (AdamW): ~38MB
  # - Attention cache: ~25MB
  # - Gradients: ~19MB
  # - Batch data and activations: ~50MB
  # Total peak VRAM: ~150MB

training:
  batch_size: 32
  learning_rate: 0.0003
  weight_decay: 0.1
  warmup_steps: 1000
  max_epochs: 50
  grad_clip: 1.0

data:
  logs_dir: "projects/love-letter/.logs"