model:
  # Model hyperparams
  vocab_size: 36              # use 36 or more if needed; check tokenizer for final count
  seq_length: 256             # maximum sequence length (left-pad if less)
  n_layer: 4
  n_head: 4
  d_model: 256
  d_ff: 1024
  dropout: 0.1
  lr: 0.0001
  batch_size: 16
  epochs: 5
  pad_token_id: 0
  device: "cuda"       # use "cuda" if available, otherwise fallback in code
  save_dir: "./checkpoints"

data:
  train_logs_dir: "../../../love-letter-logs/logs/"
  # val_logs_dir: "../../../love-letter-logs/val_logs/"   # Provide a separate dir if available